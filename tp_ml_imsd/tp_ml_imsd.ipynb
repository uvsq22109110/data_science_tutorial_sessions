{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>\n",
    "        <center><i> Data Science </i></center>\n",
    "        <center><i>tutorials session - <i style=\"color:red;\"> ML </i></i></center>\n",
    "    </h1>\n",
    "     <div style=\"float: right\">  \n",
    "         <p style=\"float: left;\"><b>Intervenant : Feki Younès &nbsp;  <br>\n",
    "                                    Year : 2020-2021  </b>\n",
    "         </p>\n",
    "      </div>\n",
    "<br><br>\n",
    "<hr>\n",
    "</center> \n",
    "\n",
    "\n",
    "### Object \n",
    "&emsp;&emsp; In this session, you are asked to solve some common problems of data science. <br>\n",
    "\n",
    "**Tutorial goals** \n",
    "\n",
    "1. Master some notions of mathematics that are behind your modeling\n",
    "2. Analyze the available datasets and make some relevant conclusions\n",
    "3. Implement a regression model\n",
    "4. Implement a classification model \n",
    "\n",
    "For each session task, you have access to code's template (functions) that you must complete. In the blank cells you should  write the answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I - Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <p style=\"color:red;\"> Numerical solution : </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <p style=\"color:red;\"> Mathematic proof : </p>\n",
    "\n",
    "<b>Write down on a paper the numerical solution of a regression problem with one feature </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <p style=\"color:red;\"> Implementation : </p>\n",
    "<b>Code the gradient descent algorithm which return the weights</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def gen_data(nb_points,bias,var):\n",
    "    '''\n",
    "        This function generate test sample :D \n",
    "    '''\n",
    "    x = np.zeros(shape=(nb_points, 2)) # [numPoints, 2] = Shape of data\n",
    "    y = np.zeros(shape=nb_points) \n",
    "    # basically a straight line\n",
    "    for i in range(0, nb_points):\n",
    "        # bias feature\n",
    "        x[i][0] = 1\n",
    "        x[i][1] = i\n",
    "        # our target variable\n",
    "        y[i] = (i + bias) + random.uniform(1, 2) * var\n",
    "    return x, y\n",
    "\n",
    "def gradient_descent(x,y,learning_rate,nb_iterations):\n",
    "    \n",
    "    weights = None\n",
    "\n",
    "    #Write code here and return weights\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent's test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/gradient_descent_numerical_sol.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# gen 100 points with a bias of 25 and 10 variance as a bit of noise\n",
    "x, y = gen_data(100, 25, 10)\n",
    "m, n = np.shape(x)\n",
    "num_iterations= 180000\n",
    "learning_rate = 0.0004\n",
    "sol1 = gradient_descent(x, y, learning_rate, num_iterations)\n",
    "print(sol1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <p style=\"color:red;\"> Vizualisation : </p>   \n",
    "\n",
    "**Plot the adjusted line with the x and y data !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_model(x, y, w):\n",
    "    '''\n",
    "        X : X Data\n",
    "        Y : Y Data\n",
    "        W : Weights\n",
    "        code à écrire ici\n",
    "    '''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/model_plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(x, y, sol1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <p style=\"color:red;\"> Analytic solution : </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <p style=\"color:red;\"> Mathematic proof : </p>\n",
    "<b>Write down on a paper the Analytic solution of a regression problem with one feature </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <p style=\"color:red;\"> Implementation : </p>\n",
    "<b>Code a function which takes X & Y as an input and returns the estimator</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_resolution(x,y):\n",
    "    \n",
    "    the_analytic_solution = \".....\"\n",
    "    return the_analytic_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./tp1_solutions/analytic_solution_gradient_descent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.....'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol2 = analytical_resolution(x, y)\n",
    "sol2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <p style=\"color:red;\"> Comparaison : </p>\n",
    "<b> Compare both solution results (arrays of coefficients) using a numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./tp1_solutions/compare_arrays.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les deux solutions sont égales\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Ecrire la réponse ici\n",
    "# Replace ''' .... ''' par la bonne fonction numpy\n",
    "####\n",
    "\n",
    "if( ''' .... '''):\n",
    "    print(\"les deux solutions sont égales\")\n",
    "else:\n",
    "    print(\"les deux solutions sont différentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p style=\"color:red;\"> Question </p> \n",
    "\n",
    "<b> In the case of a linear regression, what do you think about estimators ? Explain briefly the reasons </b> \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II  - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the link bellow, you can get the **first data set** : <a href=\"https://github.com/F3kih/Course_DS/blob/master/us_china_trade.xls?raw=true\"> <bold>link to dataset</bold> </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Import the dataset from the link without downloading it. Then, show the first 15 lines </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_china = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/import_data_trades.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Using the apply pandas function, create a column <b>Balance</b>. It is the result of soustraction of   <b>Exports & Imports</b>. After this step filter the dataset by droping July 2017 then show the last 2 rows of it</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_china[\"Balance\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/create_balance_and_filter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> <b>1)</b> Using apply affect each row to a quarter bu creating a \"Trim\" column :</p>\n",
    "the column's format is 2013-Q1 for the rows having dates in [January 2013, February 2013, March 2013]\n",
    "\n",
    "<p style=\"color:red\"> <b>2)</b> Using groupBy function, try to aggregate the actual dataset in quarterly format by summing the columns :  [Imports, Exports, Balance].<br> You have save the result in <b>us_china_trim</b> dataframe:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affect_to_quarter(month_year):\n",
    "    months = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    trim = \"...\"    \n",
    "    return trim\n",
    "\n",
    "us_china[\"Trim\"] = \"...\"\n",
    "us_china_trim = \"......\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/affect_to_quarter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> On 3 diffirent graphs, you are asked to show the evolution of the quarterly commercial activities between US and CHINA </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" ... Some code ...\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/plot_import_export_balance.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Assuming that exporation taxes of some agricultural products (respresenting 40% of the total amount of AMERICAN exportations) passed from 1.5% to 26% during the second semster of 2018 and that 65% of the AMERICAN imports from CHINA are taxed at 30%. did you see any impacts on imports and exports ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer Here :\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> When you analyse <b>Balance</b> columns, what do you think about the AMERCIAN president decision ? What did he want by applying these procedures ?</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer Here :\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III  - Data Analysis & Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the link bellow, you can get the **second data set** : <a href=\"https://github.com/F3kih/Course_DS/blob/master/train_Regression_Housing_Paris.xlsx?raw=true\"> <bold>link to dataset</bold> </a> \n",
    "<br></br>\n",
    "This dataset contains data for appart rent in PARIS. These data were scraped from LEBONCOIN between Dec 2017 - Jan 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Import the excel data (These data will be considered as the TRAIN set later) </p> <br></br>\n",
    "<b>Using a PANDAS function try to get informations about the dataset (shape and columns type)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/data_paris_train_import_explore.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">  Assuming that the column  \"is furnished\" is a boolean, do you think that the correlation between this feature and the rent has a meaningfull result ?</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Ansnwer :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> if we want to see the impact (relation) between a categorical feature and a continue feature, which test we have to choose ? Use this test to conclude about the existence of a significant relation between the rent and is furnished </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write the test code :\n",
    "'''\n",
    "\n",
    "test = \"...\"\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test name : \n",
    "Intepretation : there is (or not) a relation and why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/furnished_rent_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Use boxplot <b>(Searborn.boxplot)</b> pour mto demonstrate the statics of the rent in both cases (funished and non furnished)  </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/box_plot_rent_hue_furnished.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> using an intermediate dataframe which counts instances and 3 pie plots :   </p>\n",
    "<ol style=\"color:red\" > count the percentage of furnished and non furnished apparts </ol>\n",
    "<ol style=\"color:red\" > count the percentage of furnished and non furnished apparts rent by agencies</ol>\n",
    "<ol style=\"color:red\" > count the percentage of furnished and non furnished apparts rent by inidividuals </ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartition = \"...\"\n",
    "# plots ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/percentage_pie_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Using <b>crosstab</b> PANDAS function, retrieve the contengency table for \"is furnished\" et \"Rent from Agency\" variables  </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ecrivez le code ici\n",
    "contingency_table = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> We want to conclude about the dependecy or independency relation between  \"is furnished\" & \"Rent from agency\", which test we have to choose ? Implement it  and conclude about its results </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"....\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test name :\n",
    "interpretation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/contengency_table_and_statitic_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> We have a doubt that our problem is <b> lineaire regression </b> modeling one. We think that rent is highly correlated to surface. To confirm that (or not) which test we have to establish ? Implement it and conclude about its results. Using regplot of seaborn illustrate using a plot this relation.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "test = '...'\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test Name :\n",
    "Interpretation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regplot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/correlation_rent_area.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> For linear model, there are 3 regularization models already implemented in scikit-learn. could you remind us of these models. The difference between them if it exists. What do you think of the unicity of solution in each case.</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model 1 :\n",
    "Model 2 :\n",
    "Model 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Implement a random forest regression, a linear regression and an Elastic net models to predict the rent.</p> <br>\n",
    "\n",
    "these are caracteristics of each model : <br>\n",
    "random forest : max_depth = 2, random_state=2020, eimator number = 100  <br>\n",
    "linear regression  :  Nothing to mention <br>\n",
    "Elastic net  :  random_state=2020 <br>\n",
    "\n",
    "<p style=\"color:red\"> Train all your models </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réponse\n",
    "import sklearn\n",
    "\n",
    "# Write your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/regressions_fiting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this link you can get the Test dataset : <a href=\"https://github.com/F3kih/Course_DS/blob/master/test_Regression_Housing_Paris.xlsx?raw=true\"> <bold>link to dataset</bold> </a> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Import the testset and predict and compare your models using coefficient of determination (R^2) criteria and mean squared error (MSE)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_test = \"....\"\n",
    "\n",
    "#precition\n",
    "\n",
    "# metrics use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/regression_test_compare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part  IV  - Data analysis and classfication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the **third dataset** : <a href=\"https://github.com/F3kih/Course_DS/blob/master/umbalancedRuptcy.xls?raw=true\"> <bold>link to dataset</bold> </a> <br>\n",
    "this dataset contains bancking records. These data allow to automatically classify companies into solvent companies or not <br>\n",
    "Bellow, you can find explications of columns. Features of the dataset are resulting of an internal scoring : <br>\n",
    "<img src=\"https://github.com/F3kih/Course_DS/blob/master/dataExplain.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Import dataset and the number of each class in the target feature ? What do you think of the repartition ? Drop replicates rows in dataframe. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_banking_ruptcy = \"....\"\n",
    "# bar plot\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What do you think ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/import_car_plot_v1_banking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> 1) if you use the actual dataset to train a classification algorithm without any tunning, what will happen ? <br> 2)  Can you suggest 2 techniques that solve this problem ? (A technique using the algorithm, the other one acting on the training data ) </p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Answer 1 :\n",
    "\n",
    "\n",
    "\n",
    "- Answer 2 :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> What is the role of  the sklearn train_test_split function. What could happen if we dont stratify the split ? </p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Answer :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> We wold like to implement a linear SVM, we'll need to encode features in numeric values, to do this we will use labelEncoder scikit-learn function. Encode the dataframe and show it's header (First 5 rows)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import something ten code here\n",
    "\n",
    "data_banking_ruptcy_label_encoded = \".....\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/label_encoder_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Create a train and test sets ( 60% , 40% )  ? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ..\n",
    "# split .. random_state=2020\n",
    "x_train, x_test, y_train, y_test = \",\",\",\",\",\",\",\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp_solutions/train_test_stratify_classification_svm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Implemet a <b> linear </b> SVM , train it, test it and compute its accuracy, precision and recall. Comment the results </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "# create (random_state=2020) , fit, predict , measure\n",
    "\n",
    "clf = \"....\"\n",
    "\n",
    "# Commentaire sur les metrics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/svm_umbalanced_fit_test_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Explain briefly the bias variance tradeoff ? Waht is a model with a high variance and one with high biais  </p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le biais variance : \n",
    "\n",
    "\n",
    "modèle à haute variance :\n",
    "\n",
    "\n",
    "modèle à haut biais :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> <b>\n",
    "    In this link, we uploaded the balanced dataset version of the bank ruptcy data  <a href=\"https://github.com/F3kih/Course_DS/blob/master/balancedRuptcy.xls?raw=true\"> <bold>lien</bold> </a> . \n",
    "    You have to work with this dataset in the comming section\n",
    "    </b> <br> Import the dataset and encode its labels </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_bank_ruptcy_label_encoded = \";;\"\n",
    "# encode # drop duplicate # show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/balanced_blank_ruptcy_load_encode_remove_duplicates.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Could you remind us of the bagging and boosting technique ? Name for each of these techniques a model name ? what is the difference between these techniques ? </p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bagging :\n",
    "Bagging-model :\n",
    "\n",
    "Boosting :\n",
    "Boosting-model :\n",
    "\n",
    "Differences :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Using a train test split (60%, 40%).\n",
    "use KFOLD and  the Grid seach method on the <b> 80% of train </b> to tune your models. Choose as a metric the F1 Score.</p> <br>\n",
    "\n",
    "<p style=\"color:red\">\n",
    "\n",
    "For the models and KFOLD , please fix the  <b>random_state to 2020. </b>\n",
    "For hyperparameter tunning, this is the list of models and parameters :\n",
    "<ul>\n",
    "    <li>  <b>Random Forest </b> </li>\n",
    "        <ul>\n",
    "            <li> n_neighbors =  [ 5, 10 ] </li>\n",
    "            <li> criterion = [\"entropy\", \"gini\"]</li>\n",
    "            <li> max_depth = [1, 2] </li>\n",
    "        </ul>\n",
    "    <li> <b>AdaBoost </b></li>\n",
    "        <ul>\n",
    "            <li> n_estimators  = [5, 10, 15, 22], </li>\n",
    "            <li> learning_rate = [0.0001, 0.001, 0.01]</li>\n",
    "        </ul>\n",
    "    <li> <b>SVM  </b></li>\n",
    "        <ul>\n",
    "            <li> probability = True\n",
    "            <li> kernel = [\"rbf\",\"linear\"]</li>\n",
    "            <li> C = [1, 10, 100] </li>\n",
    "            <li> gamma = [1e-2, 1e-3, 1e-4, 1e-5] </li>\n",
    "        </ul>\n",
    "</ul>  \n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "# Split\n",
    "# Create Score\n",
    "# Prepare hyper-parameters\n",
    "# gridsearch and save best parameters\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/grid_search_tunning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Implement a <b>soft</b> voting classifier using tunned models and fit it to the train test initially splitted </p> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Voting classifier #initiate tunned classifier (set_params) # initiate soft Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/voting_fiting_predicting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Could you please explain briefly the difference between soft and hard voting ?  </p> <br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Answer : \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Implement a function which takes as input y_red and y_test label then plot the roc curve.<br> Use this function to compare the performances of the voting classifer and other classifiers </p> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_ploter(y_proba, y_test_label, model_name):\n",
    "    \n",
    "    # Improt and write your code here\n",
    "    plt.plot()\n",
    "    \n",
    "# Use this function to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/roc_curve_comparison.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"> Implement a class stacking ( voting classifier with a KNN ) to get this architecture   <br>\n",
    "<center> <img src=\"https://github.com/F3kih/Course_DS/blob/master/architecture_stacking.png?raw=true\" /> </center> <br>\n",
    "    <br> <b> plot the roc cuve for this classifier </b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking:\n",
    "    \n",
    "    def __init__(self, voting_clf):\n",
    "        print(\"ok i'm good coder\")\n",
    "        \n",
    "# init stacking\n",
    "# fit\n",
    "# predict\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp1_solutions/stacking_clf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Good luck </h1> </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
