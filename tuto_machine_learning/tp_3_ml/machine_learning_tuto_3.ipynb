{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>\n",
    "        <center><i> Data Science </i></center>\n",
    "        <center><i>tutorials session - <i style=\"color:red;\"> ML - 2 </i></i></center>\n",
    "    </h1>\n",
    "     <div style=\"float: right\">  \n",
    "         <p style=\"float: left;\"><b>Intervenant : Feki Youn√®s &nbsp;  <br>\n",
    "                                    Year : 2020-2021  </b>\n",
    "         </p>\n",
    "      </div>\n",
    "<br><br>\n",
    "<hr>\n",
    "</center> \n",
    "\n",
    "\n",
    "### Object \n",
    "&emsp;&emsp; In this session, you are asked to solve some NLP problems of data science. <br>\n",
    "\n",
    "**Tutorial goals** \n",
    "\n",
    "1. Master some notions of NLP that are behind your modeling\n",
    "2. Implement some embedding methods\n",
    "4. Implement a classification model \n",
    "\n",
    "For each session task, you have access to code's template (functions) that you must complete. In the blank cells you should  write the answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import string\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import PorterStemmer\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam & Ham Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the dataframe **spam_ham.xls** from <a href=\"https://github.com/feki-younes/data_science_tutorial_sessions/blob/master/tuto_machine_learning/tp_2_ml/spam_ham.xls?raw=true\"> link </a> and store into **df** variable.\n",
    "2. **hamonize** you text entries by setting all the **text** in **lower case**. Create a column named **text_lower** and save the result of your processing into it\n",
    "3. **Count** the number of spam and ham in your dataset.\n",
    "4. Create a list of text_lower content for each categorie (Spam & Ham) and store them into a dictionary varible ({\"cat_1\":{\"text\" : [...]},\"cat_2\":{\"text\":[...]}}\n",
    "5. Create a merged_text_lower for each category. Use the space \" \" caracter to join your text\n",
    "6. What does the Wordcloud package generate ? How it did that ? How could you interpret the size of words ? \n",
    "7. Generate a Wordcloud per category using your dictionary and 100 as the max number of words (random_state=42)\n",
    "8. What do you observe ? What is the solution for that ?\n",
    "9. What is a Tokenize in NLP ?\n",
    "10. In each category from the dictionary, create a new key **tokenized_lower_text**. The corresponding value is the tokenized lower text using a **simple white space delimiter**.\n",
    "11. What is a **stop word** ? Take the list proposed by **NTLK package** and extend it to fit with your problem\n",
    "12. Develop a function which takes a string and return it without accents and punctuation\n",
    "13. Using your **tokenized_lower_text** clean the text lists to extract only meaningful elements without accent and merge them into a string for each category and save result into **harmonized_cleaned_text** (Drop Wrokds with length 1)\n",
    "14. re-Generate a Wordcloud per category using your dictionary and 300 as the max number of words (random_state=42) \n",
    "15. Using Collection.counter get the most common 20 words for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/text_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split\n",
    "\n",
    "1. Use Sklearn Train test split and LabelEncoder and set test size to 20% (Do not forget to stratify)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/label_encoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Model\n",
    "\n",
    "1. Construct a function which take a sentence and compute the percentage of words which are in each category of your dictionary. \n",
    "2. Add a post-processing code to return the class of the category return by the function (If it return the same weight choose ramdomly a class)\n",
    "3. Add a ponderation with the position of the word in the category dictionary\n",
    "4. Test your Model (Fucntion on the test data) then compute the accuracy. What do you think ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/naive_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF & TF-IDF\n",
    "\n",
    "1. What is TF and TF-IDF \n",
    "2. What is the formula of TF-IDF \n",
    "3. Why TF-IDF is more advantageous  then TF ? What are the disadvantages \n",
    "4. Cite a distance to compute similarity between two document (Sentences) embdeddings \n",
    "5. Using sklearn library package, implement a tfidfVectorizer with your extended stop words list, Do not forget to harmonize your text (unicode, lowercase ). Set max_feature = 100\n",
    "6. print the vocabulary used by your tf-idf\n",
    "7. Using the comment at index 114 as reference compute the cosine similarity of your train data (compute the angle and the cos)\n",
    "8. display the similarity between comment at (114,119,229,710) and plot these angle ? Could you interpret these numbers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(Markdown(\"./solutions/responses_md/tf_idf.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load ./solutions/responses_py/tf_idf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transform your test text using the vectorizer\n",
    "2.  use KFOLD and  the Grid seach method on your train data. Choose as a metric the F1 Score.</p> <br>\n",
    "<p style=\"color:red\">\n",
    "For the models and KFOLD , please fix the  <b>random_state to 2020. </b>\n",
    "For hyperparameter tunning, this is the list of models and parameters :\n",
    "<ul>\n",
    "    <li>  <b>Random Forest </b> </li>\n",
    "        <ul>\n",
    "            <li> n_neighbors =  [50, 100 , 200, 400], </li>\n",
    "            <li> criterion = [\"entropy\", \"gini\"],</li>\n",
    "            <li> max_depth = [20, 30, 50] </li>\n",
    "        </ul>\n",
    "    <li> <b>MLPClassifier </b></li>\n",
    "        <ul>\n",
    "            <li> hidden_layer_sizes: [[200, 120,80],[50, 30,20],[100, 70,50]], </li>\n",
    "            <li> learning_rate : [\"constant\", \"invscaling\", \"adaptive\"] </li>\n",
    "        </ul>\n",
    "</ul>  \n",
    " </p>\n",
    " \n",
    " 3. Display a classficaition report using the best model and comapre the models\n",
    " 4. Give a simple technique that would probably improve your modelisation ? (This technique affect your TF-IDF) and explain it.\n",
    " 5. Set the ngrams to (2,3) in the tf IDF. re-train you model and export a classification report. \n",
    " 6. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(Markdown(\"./solutions/responses_md/tf_idfngrams.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/gridsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/tfidf_ngram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "1. What is Word Embedding\n",
    "2. What is Word2Vec ? How many implementation ? What is the difference ?\n",
    "3. What is Skip-Gram and CBOW ? How does it works ? Which model to choose ?\n",
    "4. What is the difference between stemming and lemmatization\n",
    "5. Load the dataset <a href=\"https://raw.githubusercontent.com/feki-younes/data_science_tutorial_sessions/master/tuto_machine_learning/tp_2_ml/fin_sentiment_analysis.csv?token=AHXAD4AFFZDYSQ25UVEDSCDAEBU74\" >link</a>. Set options :  **names=[\"sentiment\",\"news\"]**\n",
    "6. Implement a function which takes as input a text and returns the corresponding list of tokens. The function must return a lower case tokens without punctuation, numbers, stopwords and urls containing only stemmed words using Porter Stemmer. (In stopwords keep negation [no,not]). Do not forget to add bigrams and trigrams. Store this function result's in a column of your dataframe.\n",
    "7. Implement Word2Vec in Gensim **(min_count=2,window=5,size=100,iter=1000,sg=1,workers=1,seed=42)** and store its vocabulary in a variable\n",
    "8. Run  the instructions and interpret them \n",
    "    - w2v_model.most_similar(positive=['madonna', 'ibrahimovich'], negative=['swift'], topn=1)\n",
    "    - w2v_model.most_similar(positive=['ronaldo'])\n",
    "9. Transform your cleaned text in the vector space by using the mean of the news sentences\n",
    "10. Select randomly 100 sentences of each class then using PCA with 2 Components plot a graph to display you sentences in a 2D graph \n",
    "11. Display histogram of count of your class sentiment ! Transform your Sentiment class using label encoder.\n",
    "12. Split you Data into Train & Test (80%,20%)\n",
    "13. Train this model and get a classification report  **GradientBoostingClassifier(n_estimators=85,learning_rate=0.1, max_depth=4, random_state=42)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(Markdown(\"./solutions/responses_md/clean_text_w2vec.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/clean_text_w2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/w2vec_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/responses_py/w2vec_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Good luck </h1> </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
